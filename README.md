# Android Assistant App
> This repo contains all the code and the build for the android app developed in the video. It is not thoroughly tested and may be prone to bugs as this was 100% vibe coded from someone (me) who has never developed on android before this repo.
## Installation
1. To use the app, you must download a compatible .litertlm model on your phone (device), the one used in the video is from: [gemma-3n-E4B-it-int4.litertlm](https://huggingface.co/google/gemma-3n-E4B-it-litert-lm/tree/main) (others may work if they are compatible with google's [mediapipe](https://ai.google.dev/edge/mediapipe/solutions/guide), I haven't tried).
2. Go to the [releases](https://github.com/JarodMica/android_assistant/releases/tag/v0.1) section and download and install the `app-debug.apk` in the Assets area.
3. Once you have the app, you can use it with .litertlm model (gemma-3n) from earlier for multimodal inference.
4. To enable TTS, you will need a Murf AI API key that you can get with an account from: https://murf.ai/falcon. Once you have one, you can place it in the TTS Settings tab of the app and it should allow you to use it with the default voice.

